version: "3.9"

services:
  db:
    image: mysql:8.0
    container_name: redmine-db
    restart: always
    environment:
      MYSQL_DATABASE: redmine_db
      MYSQL_USER: redmine
      MYSQL_PASSWORD: enter_password
      MYSQL_ROOT_PASSWORD: Enter_Password!
    volumes:
      - mysql_data:/var/lib/mysql
    networks: [ai_net]
    platform: linux/arm64

  redmine:
    build:
      context: ./redmine
      dockerfile: Dockerfile
    container_name: redmine-app
    restart: always
    ports:
      - "3000:3000"
    depends_on:
      - db
    environment:
      BUNDLE_FORCE_RUBY_PLATFORM: "true"
      REDMINE_HOST: codedeck.local
      REDMINE_HTTPS: "true"
      REDMINE_DB_MYSQL: db
      REDMINE_DB_PORT: 3306
      REDMINE_DB_DATABASE: redmine_db
      REDMINE_DB_USERNAME: redmine
      REDMINE_DB_PASSWORD: enter_password
      # Use real secrets in production; generate once and paste:
      # rails secret   OR   openssl rand -hex 64
      RAILS_SECRET_KEY_BASE: "REPLACE_ME"
      SECRET_KEY_BASE: "REPLACE_ME"
    volumes:
      - redmine_data:/usr/src/redmine/files
      - ./redmine/config/database.yml:/usr/src/redmine/config/database.yml
      - ./redmine/plugins:/usr/src/redmine/plugins
      - ./redmine/themes:/usr/src/redmine/public/themes
      - ./redmine/redmine-init.sh:/usr/local/bin/redmine-init.sh
    entrypoint: ["/usr/local/bin/redmine-init.sh"]
    command: ["rails", "server", "-b", "0.0.0.0", "-p", "3000"]
    networks: [ai_net]
    platform: linux/arm64

  n8n:
    build:
      context: ./n8n
    container_name: n8n
    restart: always
    ports:
      - "5678:5678"
    environment:
      - N8N_CUSTOM_EXTENSIONS=/data/packages
      - N8N_HOST=n8n.codemusic.ca
      - N8N_PROTOCOL=http
      - N8N_PORT=5678
      - N8N_WEBHOOK_URL=http://codedeck.local:5678/
      - WEBHOOK_URL=http://codedeck.local:5678/
      - MORTY_SIGN_KEY=q9ofLSY/cc96sOWKqpttFYEfHBD8nhxIun/NfOon2qA=
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/plugins:/data/packages
    networks: [ai_net]
    platform: linux/arm64

  # ========= OLLAMA (ARM64) =========
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      OLLAMA_NUM_PARALLEL: "1"
      OLLAMA_KEEP_ALIVE: "30m"
    volumes:
      - ollama_models:/root/.ollama
    networks: [ai_net]
    platform: linux/arm64
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 20s
      timeout: 5s
      retries: 10

  # ========= MUSAI FRONTEND =========
  musai:
    build:
      context: ./musai
    container_name: musai
    restart: always
    ports:
      - "5680:80"
    environment:
      - VITE_BASE_PATH=/
      - VITE_N8N_WEBHOOK_URL=http://codedeck.local:5678/webhook/
      - VITE_WELCOME_MESSAGE=Welcome to Musai
      - VITE_SITE_TITLE=Musai
      - VITE_N8N_WEBHOOK_USERNAME=codemusai
      - VITE_N8N_WEBHOOK_SECRET=R0V3RBY73
      - VITE_ASSISTANT_NAME=Musai
      - VITE_RIDDLE_GATE_MODE=preview
    networks: [ai_net]
    platform: linux/arm64

  # ========= MUSAI API (uses internal Ollama) =========
  musai-api:
    build: ./musai-api
    container_name: musai-api
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      VOICES_DIR: /voices
      WHISPER_MODELS_DIR: /whisper-models
      FASTER_WHISPER_MODEL: base.en
      DEFAULT_PIPER_VOICE: en_US-GlaDOS-medium
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: llama3
    volumes:
      - ./ai-model-cache/piper-voices:/voices:ro
      - ./ai-model-cache/whisper-models:/whisper-models:ro
    depends_on:
      - ollama
    networks: [ai_net]
    platform: linux/arm64

  # ========= COMFYUI (ARM64 build) =========
  comfyui:
    build:
      context: ./comfyui
      dockerfile: Dockerfile.comfyui    # create this for ARM64 (see notes below)
    container_name: musai_comfyui
    restart: unless-stopped
    ports:
      - "8008:8188"
    volumes:
      - ./ai-model-cache/comfyui-models:/root/ComfyUI/models
      - ./ai-model-cache/comfyui-output:/root/ComfyUI/output
      - ./ai-model-cache/comfyui-input:/root/ComfyUI/input
    environment:
      # Pi has no CUDA; you'll be running CPU/OpenCL/Vulkan backends.
      COMFYUI_ARGS: --listen 0.0.0.0 --port 8188
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8188/"]
      interval: 30s
      timeout: 8s
      retries: 10
    networks: [ai_net]
    platform: linux/arm64

  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    ports:
      - "8080:8080"
    depends_on: [morty]
    networks: [ai_net]
    volumes:
      - ./searxng/settings.yml:/etc/searxng/settings.yml
      - ./searxng/branding/img:/usr/local/searxng/searx/static/themes/simple/img:ro
      - ./searxng/static/css:/usr/local/searxng/searx/static/themes/simple/css:ro
      - ./searxng/static/js:/usr/local/searxng/searx/static/themes/simple/js:ro
      - ./searxng/templates/simple:/usr/local/searxng/searx/templates/simple:ro
    healthcheck:
      test: ["CMD", "wget", "-qO", "-", "http://localhost:8080/search?q=ping&format=json"]
      interval: 30s
      timeout: 5s
      retries: 5
    platform: linux/arm64

  morty:
    image: dalf/morty:latest
    container_name: morty
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - MORTY_KEY=q9ofLSY/cc96sOWKqpttFYEfHBD8nhxIun/NfOon2qA=
      - MORTY_ADDRESS=0.0.0.0:3000
      - DEBUG=true
    networks: [ai_net]
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/ >/dev/null || exit 1"]
      interval: 20s
      timeout: 5s
      retries: 10
    platform: linux/arm64

  wyoming-whisper:
    image: rhasspy/wyoming-whisper
    restart: unless-stopped
    command: --model base.en --language en --uri tcp://0.0.0.0:10300
    ports: ["10300:10300"]
    networks:
      ai_net:
        aliases: [wyoming-whisper]
    platform: linux/arm64

  wyoming-piper:
    image: rhasspy/wyoming-piper
    restart: unless-stopped
    volumes:
      - ./ai-model-cache/piper-voices:/voices
    command: >
      --model  /voices/en_US-GlaDOS-medium.onnx
      --config /voices/en_US-GlaDOS-medium.onnx.json
      --uri    tcp://0.0.0.0:10200
    ports: ["10200:10200"]
    networks:
      ai_net:
        aliases: [wyoming-piper]
    platform: linux/arm64

  silverbullet:
    build:
      context: .
      dockerfile: silverbullet/Dockerfile
    container_name: rovervault
    restart: unless-stopped
    ports:
      - "3030:3000"
    volumes:
      - ./silverbullet/data/notes:/space
      - ./silverbullet/PLUGINS/silverbullet-ai:/space/PLUGINS/silverbullet-ai
      - ./silverbullet/SECRETS:/space/SECRETS
      - ./silverbullet/SETTINGS:/space/SETTINGS
    environment:
      - BASE_URL=http://localhost:3030
      - SB_USER=admin:EnterPasswordHere!!
    networks: [ai_net]
    platform: linux/arm64

  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    ports:
      - "8123:8123"
    volumes:
      - ./homeassist-configs/dev-config:/config
      - /etc/localtime:/etc/localtime:ro
    networks: [ai_net]
    platform: linux/arm64

volumes:
  mysql_data:
  redmine_data:
  n8n_data:
  ollama_models:

networks:
  ai_net:
    driver: bridge